{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bea4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"EMAIL\"] , os.environ[\"PASSWORD\"]\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "email = driver.find_element(By.ID, 'username')\n",
    "email.send_keys(os.environ['EMAIL'])\n",
    "\n",
    "password = driver.find_element(By.ID, 'password')\n",
    "password.send_keys(os.environ['PASSWORD'])\n",
    "\n",
    "password.submit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.linkedin.com/in/laxmimerit\"\n",
    "driver.get(url)\n",
    "\n",
    "profile_data = {}\n",
    "page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "name = soup.find('h1', {'class': 'UXwXGvkZjLHXTkTzfStIRtZBcIdwKURDfTbmzc inline t-24 v-align-middle break-words'})\n",
    "\n",
    "name = name.get_text().strip()\n",
    "\n",
    "profile_data['name'] = name\n",
    "profile_data['url'] = url\n",
    "\n",
    "headline = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "headline = headline.get_text().strip()\n",
    "\n",
    "profile_data['headline'] = headline\n",
    "\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "about = soup.find('div', {'class': 'display-flex ph5 pv3'})\n",
    "\n",
    "about = about.get_text().strip()\n",
    "\n",
    "profile_data['about'] = about\n",
    "\n",
    "#Certifications\n",
    "\n",
    "driver.find_element(By.ID, \"navigation-index-see-all-licenses-and-certifications\").click()\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "soup = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "sleep(10)\n",
    "soup\n",
    "items = soup.find_all('div', {'class': 'HenMyUvhyygNwObsjLGfQGNCClskwiJeUJcs vgkiOMXEtuuAEHRhvoEHYENhpnnldfMcajXg rdDTnlcSepkFmmtIZnVBvXIrlCsjevNXWhvLE'})\n",
    "len(items)\n",
    "item = items[0]\n",
    "\n",
    "def get_license(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "    item_dict = {}\n",
    "    item_dict['name'] = spans[0].get_text().strip()\n",
    "    item_dict['institute'] = spans[1].get_text().strip()\n",
    "    item_dict['issued_date'] = spans[2].get_text().strip()\n",
    "\n",
    "    return item_dict\n",
    "\n",
    "item_list = []\n",
    "for item in items:\n",
    "    item_list.append(get_license(item))\n",
    "\n",
    "profile_data['licenses'] = item_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349f08c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f853c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Laxmi Kant Tiwari',\n",
       " 'url': 'https://www.linkedin.com/in/laxmimerit',\n",
       " 'headline': 'Gen AI in Finance & Investment Services | Data Scientist | IIT Kharagpur | Asset Management | AI-Driven Financial Modeling | Search Ranking | NLP Python BERT AWS Elasticsearch GNN SQL LLM | AI in Investment Strategies',\n",
       " 'about': 'Demonstrated 8+ years of expertise in Advanced Analytics and Machine Learning, currently leading strategic AI initiatives at Linedata. I specialize in designing scalable GenAI and LLM-driven applications for financial workflows, transforming legacy processes into intelligent systems.ðŸ”¹ Key Projects in Finance AI at Linedataâ–ª Trade Reconciliation Break Analysis for Cash and Position BreaksDeveloped an AI-powered reconciliation system that automates the detection and analysis of cash and position breaks, enhancing operational efficiency and ensuring compliance.â–ª Security and Account AliasingBuilt a robust aliasing framework using custom algorithms to map securities and accounts across multiple systems, reducing operational risk and improving data integrity.â–ª Finance Memo Builder for Financial DocumentsDesigned a scalable system using LLMs to auto-generate detailed financial memos, drastically improving reporting accuracy and speed.â–ª Research Assistants for Security DocumentsImplemented LLM-based assistants that extract and summarize key insights from dense security documents, accelerating research and analysis.â–ª Covenants Analysis for Credit AgreementsDelivered a custom NLP solution that interprets covenants from credit agreements, providing transparent risk insights to legal and finance teams.ðŸ”¹ LLM + GenAI Projects (Open Source)â–ª Agentic RAG Systems with LangGraphBuilt dynamic agent-based chat flows using graph execution models for adaptive RAG systems in enterprise-scale settings.â–ª Resume Parsing and DeploymentDeveloped a production-grade resume parser powered by LLMs, including entity extraction and live deployment through a web interface.â–ª Chatbot with Vector DB + MemoryCreated LLM-driven chatbots that maintain contextual memory, retrieve information from vector stores, and respond with high accuracy.â–ª Text-to-SQL ChatbotBuilt an LLM system to convert natural language queries into SQL, enabling business users to interact with databases conversationally.â–ª LinkedIn Profile AnalyzerScraped and processed LinkedIn profiles using LLMs to extract professional summaries, skills, and experience insights.â–ª Financial Document Analyst using DeepSeekImplemented local RAG solutions for parsing quarterly earnings, balance sheets, and analyst commentaries using DeepSeek R1.â–ª PDF Chatbot with HistoryBuilt multi-turn chatbots that answer user questions based on local documents and preserve interaction context.Demonstrated 8+ years of expertise in Advanced Analytics and Machine Learning, currently leading strategic AI initiatives at Linedata. I specialize in designing scalable GenAI and LLM-driven applications for financial workflows, transforming legacy processes into intelligent systems.\\n\\nðŸ”¹ Key Projects in Finance AI at Linedata\\nâ–ª Trade Reconciliation Break Analysis for Cash and Position Breaks\\nDeveloped an AI-powered reconciliation system that automates the detection and analysis of cash and position breaks, enhancing operational efficiency and ensuring compliance.\\n\\nâ–ª Security and Account Aliasing\\nBuilt a robust aliasing framework using custom algorithms to map securities and accounts across multiple systems, reducing operational risk and improving data integrity.\\n\\nâ–ª Finance Memo Builder for Financial Documents\\nDesigned a scalable system using LLMs to auto-generate detailed financial memos, drastically improving reporting accuracy and speed.\\n\\nâ–ª Research Assistants for Security Documents\\nImplemented LLM-based assistants that extract and summarize key insights from dense security documents, accelerating research and analysis.\\n\\nâ–ª Covenants Analysis for Credit Agreements\\nDelivered a custom NLP solution that interprets covenants from credit agreements, providing transparent risk insights to legal and finance teams.\\n\\nðŸ”¹ LLM + GenAI Projects (Open Source)\\nâ–ª Agentic RAG Systems with LangGraph\\nBuilt dynamic agent-based chat flows using graph execution models for adaptive RAG systems in enterprise-scale settings.\\n\\nâ–ª Resume Parsing and Deployment\\nDeveloped a production-grade resume parser powered by LLMs, including entity extraction and live deployment through a web interface.\\n\\nâ–ª Chatbot with Vector DB + Memory\\nCreated LLM-driven chatbots that maintain contextual memory, retrieve information from vector stores, and respond with high accuracy.\\n\\nâ–ª Text-to-SQL Chatbot\\nBuilt an LLM system to convert natural language queries into SQL, enabling business users to interact with databases conversationally.\\n\\nâ–ª LinkedIn Profile Analyzer\\nScraped and processed LinkedIn profiles using LLMs to extract professional summaries, skills, and experience insights.\\n\\nâ–ª Financial Document Analyst using DeepSeek\\nImplemented local RAG solutions for parsing quarterly earnings, balance sheets, and analyst commentaries using DeepSeek R1.\\n\\nâ–ª PDF Chatbot with History\\nBuilt multi-turn chatbots that answer user questions based on local documents and preserve interaction context.',\n",
       " 'licenses': [{'name': 'Product Analytics Micro Certificate',\n",
       "   'institute': 'Product School',\n",
       "   'issued_date': 'Issued Jan 2024'},\n",
       "  {'name': 'Accenture Innovation Award',\n",
       "   'institute': 'Accenture',\n",
       "   'issued_date': 'Issued Nov 2017'},\n",
       "  {'name': 'Top 50 Innovative Startup of India, India Innovation Growth Programme',\n",
       "   'institute': 'Government of India',\n",
       "   'issued_date': 'Issued Jun 2017'},\n",
       "  {'name': 'Top 30 Technical Innovative Startup of India, IICDC 2016',\n",
       "   'institute': 'Texas Instruments',\n",
       "   'issued_date': 'Issued May 2017'},\n",
       "  {'name': 'Best Startup at IIT Kharagpur 2016-17',\n",
       "   'institute': 'IIT Kharagpur',\n",
       "   'issued_date': 'Issued Feb 2017'},\n",
       "  {'name': 'Winner of the International Business Model Competition, Eureka 2016',\n",
       "   'institute': 'IIT Mumbai',\n",
       "   'issued_date': 'Issued Jan 2017'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c57c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m soup = BeautifulSoup(page_source, \u001b[33m'\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     39\u001b[39m soup = soup.find(\u001b[33m'\u001b[39m\u001b[33msection\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33martdeco-card pb3\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m items = \u001b[43msoup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_all\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mHenMyUvhyygNwObsjLGfQGNCClskwiJeUJcs vgkiOMXEtuuAEHRhvoEHYENhpnnldfMcajXg rdDTnlcSepkFmmtIZnVBvXIrlCsjevNXWhvLE\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     43\u001b[39m item = items[\u001b[32m0\u001b[39m]\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_course\u001b[39m(item):\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#projects\n",
    "\n",
    "driver.back()\n",
    "driver.find_element(By.ID, \"navigation-index-see-all-projects\").click()\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "soup = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "sleep(10)\n",
    "soup\n",
    "\n",
    "items = soup.find_all('div', {'class': 'HenMyUvhyygNwObsjLGfQGNCClskwiJeUJcs vgkiOMXEtuuAEHRhvoEHYENhpnnldfMcajXg rdDTnlcSepkFmmtIZnVBvXIrlCsjevNXWhvLE'})\n",
    "len(items)\n",
    "\n",
    "item = items[0]\n",
    "\n",
    "def get_project(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "    item_dict = {}\n",
    "    item_dict['project_name'] = spans[0].get_text().strip()\n",
    "    item_dict['duration'] = spans[1].get_text().strip()\n",
    "    item_dict['description'] = spans[2].get_text().strip()\n",
    "\n",
    "    return item_dict\n",
    "\n",
    "item_list = []\n",
    "for item in items:\n",
    "    item_list.append(get_project(item))\n",
    "\n",
    "profile_data['projects'] = item_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8fd655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Laxmi Kant Tiwari',\n",
       " 'url': 'https://www.linkedin.com/in/laxmimerit',\n",
       " 'headline': 'Gen AI in Finance & Investment Services | Data Scientist | IIT Kharagpur | Asset Management | AI-Driven Financial Modeling | Search Ranking | NLP Python BERT AWS Elasticsearch GNN SQL LLM | AI in Investment Strategies',\n",
       " 'about': 'Demonstrated 8+ years of expertise in Advanced Analytics and Machine Learning, currently leading strategic AI initiatives at Linedata. I specialize in designing scalable GenAI and LLM-driven applications for financial workflows, transforming legacy processes into intelligent systems.ðŸ”¹ Key Projects in Finance AI at Linedataâ–ª Trade Reconciliation Break Analysis for Cash and Position BreaksDeveloped an AI-powered reconciliation system that automates the detection and analysis of cash and position breaks, enhancing operational efficiency and ensuring compliance.â–ª Security and Account AliasingBuilt a robust aliasing framework using custom algorithms to map securities and accounts across multiple systems, reducing operational risk and improving data integrity.â–ª Finance Memo Builder for Financial DocumentsDesigned a scalable system using LLMs to auto-generate detailed financial memos, drastically improving reporting accuracy and speed.â–ª Research Assistants for Security DocumentsImplemented LLM-based assistants that extract and summarize key insights from dense security documents, accelerating research and analysis.â–ª Covenants Analysis for Credit AgreementsDelivered a custom NLP solution that interprets covenants from credit agreements, providing transparent risk insights to legal and finance teams.ðŸ”¹ LLM + GenAI Projects (Open Source)â–ª Agentic RAG Systems with LangGraphBuilt dynamic agent-based chat flows using graph execution models for adaptive RAG systems in enterprise-scale settings.â–ª Resume Parsing and DeploymentDeveloped a production-grade resume parser powered by LLMs, including entity extraction and live deployment through a web interface.â–ª Chatbot with Vector DB + MemoryCreated LLM-driven chatbots that maintain contextual memory, retrieve information from vector stores, and respond with high accuracy.â–ª Text-to-SQL ChatbotBuilt an LLM system to convert natural language queries into SQL, enabling business users to interact with databases conversationally.â–ª LinkedIn Profile AnalyzerScraped and processed LinkedIn profiles using LLMs to extract professional summaries, skills, and experience insights.â–ª Financial Document Analyst using DeepSeekImplemented local RAG solutions for parsing quarterly earnings, balance sheets, and analyst commentaries using DeepSeek R1.â–ª PDF Chatbot with HistoryBuilt multi-turn chatbots that answer user questions based on local documents and preserve interaction context.Demonstrated 8+ years of expertise in Advanced Analytics and Machine Learning, currently leading strategic AI initiatives at Linedata. I specialize in designing scalable GenAI and LLM-driven applications for financial workflows, transforming legacy processes into intelligent systems.\\n\\nðŸ”¹ Key Projects in Finance AI at Linedata\\nâ–ª Trade Reconciliation Break Analysis for Cash and Position Breaks\\nDeveloped an AI-powered reconciliation system that automates the detection and analysis of cash and position breaks, enhancing operational efficiency and ensuring compliance.\\n\\nâ–ª Security and Account Aliasing\\nBuilt a robust aliasing framework using custom algorithms to map securities and accounts across multiple systems, reducing operational risk and improving data integrity.\\n\\nâ–ª Finance Memo Builder for Financial Documents\\nDesigned a scalable system using LLMs to auto-generate detailed financial memos, drastically improving reporting accuracy and speed.\\n\\nâ–ª Research Assistants for Security Documents\\nImplemented LLM-based assistants that extract and summarize key insights from dense security documents, accelerating research and analysis.\\n\\nâ–ª Covenants Analysis for Credit Agreements\\nDelivered a custom NLP solution that interprets covenants from credit agreements, providing transparent risk insights to legal and finance teams.\\n\\nðŸ”¹ LLM + GenAI Projects (Open Source)\\nâ–ª Agentic RAG Systems with LangGraph\\nBuilt dynamic agent-based chat flows using graph execution models for adaptive RAG systems in enterprise-scale settings.\\n\\nâ–ª Resume Parsing and Deployment\\nDeveloped a production-grade resume parser powered by LLMs, including entity extraction and live deployment through a web interface.\\n\\nâ–ª Chatbot with Vector DB + Memory\\nCreated LLM-driven chatbots that maintain contextual memory, retrieve information from vector stores, and respond with high accuracy.\\n\\nâ–ª Text-to-SQL Chatbot\\nBuilt an LLM system to convert natural language queries into SQL, enabling business users to interact with databases conversationally.\\n\\nâ–ª LinkedIn Profile Analyzer\\nScraped and processed LinkedIn profiles using LLMs to extract professional summaries, skills, and experience insights.\\n\\nâ–ª Financial Document Analyst using DeepSeek\\nImplemented local RAG solutions for parsing quarterly earnings, balance sheets, and analyst commentaries using DeepSeek R1.\\n\\nâ–ª PDF Chatbot with History\\nBuilt multi-turn chatbots that answer user questions based on local documents and preserve interaction context.',\n",
       " 'licenses': [{'name': 'Product Analytics Micro Certificate',\n",
       "   'institute': 'Product School',\n",
       "   'issued_date': 'Issued Jan 2024'},\n",
       "  {'name': 'Accenture Innovation Award',\n",
       "   'institute': 'Accenture',\n",
       "   'issued_date': 'Issued Nov 2017'},\n",
       "  {'name': 'Top 50 Innovative Startup of India, India Innovation Growth Programme',\n",
       "   'institute': 'Government of India',\n",
       "   'issued_date': 'Issued Jun 2017'},\n",
       "  {'name': 'Top 30 Technical Innovative Startup of India, IICDC 2016',\n",
       "   'institute': 'Texas Instruments',\n",
       "   'issued_date': 'Issued May 2017'},\n",
       "  {'name': 'Best Startup at IIT Kharagpur 2016-17',\n",
       "   'institute': 'IIT Kharagpur',\n",
       "   'issued_date': 'Issued Feb 2017'},\n",
       "  {'name': 'Winner of the International Business Model Competition, Eureka 2016',\n",
       "   'institute': 'IIT Mumbai',\n",
       "   'issued_date': 'Issued Jan 2017'}],\n",
       " 'projects': [{'project_name': 'Smart Sleep Sense: An Unobtrusive and IoT Enabled Wireless Sleep Monitoring System',\n",
       "   'duration': 'May 2015 - Present',\n",
       "   'description': 'The project aims at developing a portable Human monitoring device that detects the health parameters (heart pulse, breathing rate, thermal emotion recognition, awakening at REM stage of the sleep) and communicates the signals obtained from sensors to the Android app via Wireless network. The information is used to convey Smart Assistance to the user for their daily improved health care conditions thus improving their efficiency and longevity. We are offering sleep disorder monitoring devices. In addition to this, our business model is B2C and device will be directly available to the customer for purchase via online e-commerce stores as well as company e-store itself and the expected cost will be in between 110 to 149 dollar.'},\n",
       "  {'project_name': 'Optimized Fixed Point Kalman Filter Implementation on Embedded Hardware',\n",
       "   'duration': 'Aug 2013 - Apr 2014',\n",
       "   'description': 'Real time adaptive signal processing is an area of considerable interest for the researchers in the field of embedded signal processing. Owing to the large dynamic range and constant relative accuracy of floating point arithmetic, filters are traditionally realized in floating point data type. However, using floating point arithmetic in embedded systems lead to larger cost in terms of latency, power and memory. Therefore these algorithms need to be implemented in fixed point for optimizing these constraints. However with the reduction in the word length the precision get affected. Therefore these algorithms need to be coded properly with adequate fixed point format such that precision is not affected while the resources are optimized. In this thesis an attempt has been made to implement Least Mean Square (LMS), Recursive Least Square (RLS) and Kalman Filtering algorithms on ARM Cortex M0+ with Thumb instruction set. The word length has been optimized for accurate performance, reduced power, improved speed and reduced memory usage. The validation of the algorithm has been carried out with respect to MATLAB.'},\n",
       "  {'project_name': 'MIPS64 5-stages Processor Designing',\n",
       "   'duration': 'Jan 2013 - May 2013',\n",
       "   'description': 'The machine was designed with the MIPS instruction set in mind. Its purpose was to be able to take  a  binary  representation  of  a  MIPS  instruction  and  convert  it  into  microcode.    The microcode controls the actions of the processor and pipeline. Pipelining is a process of multitasking instructions in the same data path.  An instruction can be loaded into the data path at every clock cycle, even though each instruction takes up to 5 cycles to complete.  At every clock cycle, each instruction is stored into a different stage in the pipeline. Doing this does not affect any of the other instructions in the processor because they are in different stages. Each stage  contains  a  different  part  of  the  data  path  and  has  a specific function. This allows the user of the processor to fully utilize all components of the data path simultaneously, causing an increase in the throughput of their program.'},\n",
       "  {'project_name': 'Transistor Characteristics Curve Tracer',\n",
       "   'duration': 'Feb 2012 - May 2013',\n",
       "   'description': 'A cost effective and reliable curve tracer has been designed and developed for NPN- PNP transistors using AT89C51 microcontroller. The system plots desired number of transfer and static characteristics curves on a given range of constant biasing parameter viz; VCE and IB resp. The system plots these characteristics curves of the given transistor on the available computer system through the developed software on Visual BASIC 6.0.  h-parameters of the transistor have also been obtained for different design requirement. All the data has also been stored in a user defined output file for further analysis.'},\n",
       "  {'project_name': 'Automated BJT Curve Tracer using Visual Basic',\n",
       "   'duration': 'Jan 2012 - May 2012',\n",
       "   'description': 'The characteristic of any discrete component plays an important role for determining its behavior for different applications. In this paper we have developed a cost effective BJT curve tracer and binning system which consist of hardware part that is implemented using 89c51 microcontroller and software part which is developed in Microsoft Visual Basic 6.0. This complete system plots the input characteristics curves, h-parameters of the given transistor along with other parameters i.e. C-E resistance and early voltage. Binning machine is used to find the working mode of BJT and to find whether the BJT is working or not by using the port given on the hardware. All the data can also be stored in a user defined output file in the computer for further analysis. This tracer and binning machine can be used for educational and engineering purposes.'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5e097",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m soup = BeautifulSoup(page_source, \u001b[33m'\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     39\u001b[39m soup = soup.find(\u001b[33m'\u001b[39m\u001b[33msection\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33martdeco-card pb3\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m items = \u001b[43msoup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_all\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mspan\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mvisually-hidden\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     43\u001b[39m item_list = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "driver.back()\n",
    "\n",
    "driver.find_element(By.ID, \"navigation-index-see-all-courses\").click()\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "soup = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "\n",
    "items = soup.find_all('div', {'class': 'HenMyUvhyygNwObsjLGfQGNCClskwiJeUJcs vgkiOMXEtuuAEHRhvoEHYENhpnnldfMcajXg rdDTnlcSepkFmmtIZnVBvXIrlCsjevNXWhvLE'})\n",
    "\n",
    "item = items[0]\n",
    "\n",
    "def get_course(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "    item_dict = {}\n",
    "    item_dict['course_name'] = spans[0].get_text().strip()\n",
    "    try:\n",
    "        item_dict['associated_with'] = spans[1].get_text().strip()\n",
    "    except:\n",
    "        item_dict['associated_with'] = \"\"\n",
    "\n",
    "    return item_dict\n",
    "\n",
    "item_list = []\n",
    "for item in items:\n",
    "    item_list.append(get_course(item))\n",
    "\n",
    "profile_data['courses'] = item_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e8c180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Laxmi Kant Tiwari',\n",
       " 'url': 'https://www.linkedin.com/in/laxmimerit',\n",
       " 'headline': 'Gen AI in Finance & Investment Services | Data Scientist | IIT Kharagpur | Asset Management | AI-Driven Financial Modeling | Search Ranking | NLP Python BERT AWS Elasticsearch GNN SQL LLM | AI in Investment Strategies',\n",
       " 'about': 'Demonstrated 8+ years of expertise in Advanced Analytics and Machine Learning, currently leading strategic AI initiatives at Linedata. I specialize in designing scalable GenAI and LLM-driven applications for financial workflows, transforming legacy processes into intelligent systems.ðŸ”¹ Key Projects in Finance AI at Linedataâ–ª Trade Reconciliation Break Analysis for Cash and Position BreaksDeveloped an AI-powered reconciliation system that automates the detection and analysis of cash and position breaks, enhancing operational efficiency and ensuring compliance.â–ª Security and Account AliasingBuilt a robust aliasing framework using custom algorithms to map securities and accounts across multiple systems, reducing operational risk and improving data integrity.â–ª Finance Memo Builder for Financial DocumentsDesigned a scalable system using LLMs to auto-generate detailed financial memos, drastically improving reporting accuracy and speed.â–ª Research Assistants for Security DocumentsImplemented LLM-based assistants that extract and summarize key insights from dense security documents, accelerating research and analysis.â–ª Covenants Analysis for Credit AgreementsDelivered a custom NLP solution that interprets covenants from credit agreements, providing transparent risk insights to legal and finance teams.ðŸ”¹ LLM + GenAI Projects (Open Source)â–ª Agentic RAG Systems with LangGraphBuilt dynamic agent-based chat flows using graph execution models for adaptive RAG systems in enterprise-scale settings.â–ª Resume Parsing and DeploymentDeveloped a production-grade resume parser powered by LLMs, including entity extraction and live deployment through a web interface.â–ª Chatbot with Vector DB + MemoryCreated LLM-driven chatbots that maintain contextual memory, retrieve information from vector stores, and respond with high accuracy.â–ª Text-to-SQL ChatbotBuilt an LLM system to convert natural language queries into SQL, enabling business users to interact with databases conversationally.â–ª LinkedIn Profile AnalyzerScraped and processed LinkedIn profiles using LLMs to extract professional summaries, skills, and experience insights.â–ª Financial Document Analyst using DeepSeekImplemented local RAG solutions for parsing quarterly earnings, balance sheets, and analyst commentaries using DeepSeek R1.â–ª PDF Chatbot with HistoryBuilt multi-turn chatbots that answer user questions based on local documents and preserve interaction context.Demonstrated 8+ years of expertise in Advanced Analytics and Machine Learning, currently leading strategic AI initiatives at Linedata. I specialize in designing scalable GenAI and LLM-driven applications for financial workflows, transforming legacy processes into intelligent systems.\\n\\nðŸ”¹ Key Projects in Finance AI at Linedata\\nâ–ª Trade Reconciliation Break Analysis for Cash and Position Breaks\\nDeveloped an AI-powered reconciliation system that automates the detection and analysis of cash and position breaks, enhancing operational efficiency and ensuring compliance.\\n\\nâ–ª Security and Account Aliasing\\nBuilt a robust aliasing framework using custom algorithms to map securities and accounts across multiple systems, reducing operational risk and improving data integrity.\\n\\nâ–ª Finance Memo Builder for Financial Documents\\nDesigned a scalable system using LLMs to auto-generate detailed financial memos, drastically improving reporting accuracy and speed.\\n\\nâ–ª Research Assistants for Security Documents\\nImplemented LLM-based assistants that extract and summarize key insights from dense security documents, accelerating research and analysis.\\n\\nâ–ª Covenants Analysis for Credit Agreements\\nDelivered a custom NLP solution that interprets covenants from credit agreements, providing transparent risk insights to legal and finance teams.\\n\\nðŸ”¹ LLM + GenAI Projects (Open Source)\\nâ–ª Agentic RAG Systems with LangGraph\\nBuilt dynamic agent-based chat flows using graph execution models for adaptive RAG systems in enterprise-scale settings.\\n\\nâ–ª Resume Parsing and Deployment\\nDeveloped a production-grade resume parser powered by LLMs, including entity extraction and live deployment through a web interface.\\n\\nâ–ª Chatbot with Vector DB + Memory\\nCreated LLM-driven chatbots that maintain contextual memory, retrieve information from vector stores, and respond with high accuracy.\\n\\nâ–ª Text-to-SQL Chatbot\\nBuilt an LLM system to convert natural language queries into SQL, enabling business users to interact with databases conversationally.\\n\\nâ–ª LinkedIn Profile Analyzer\\nScraped and processed LinkedIn profiles using LLMs to extract professional summaries, skills, and experience insights.\\n\\nâ–ª Financial Document Analyst using DeepSeek\\nImplemented local RAG solutions for parsing quarterly earnings, balance sheets, and analyst commentaries using DeepSeek R1.\\n\\nâ–ª PDF Chatbot with History\\nBuilt multi-turn chatbots that answer user questions based on local documents and preserve interaction context.',\n",
       " 'licenses': [{'name': 'Product Analytics Micro Certificate',\n",
       "   'institute': 'Product School',\n",
       "   'issued_date': 'Issued Jan 2024'},\n",
       "  {'name': 'Accenture Innovation Award',\n",
       "   'institute': 'Accenture',\n",
       "   'issued_date': 'Issued Nov 2017'},\n",
       "  {'name': 'Top 50 Innovative Startup of India, India Innovation Growth Programme',\n",
       "   'institute': 'Government of India',\n",
       "   'issued_date': 'Issued Jun 2017'},\n",
       "  {'name': 'Top 30 Technical Innovative Startup of India, IICDC 2016',\n",
       "   'institute': 'Texas Instruments',\n",
       "   'issued_date': 'Issued May 2017'},\n",
       "  {'name': 'Best Startup at IIT Kharagpur 2016-17',\n",
       "   'institute': 'IIT Kharagpur',\n",
       "   'issued_date': 'Issued Feb 2017'},\n",
       "  {'name': 'Winner of the International Business Model Competition, Eureka 2016',\n",
       "   'institute': 'IIT Mumbai',\n",
       "   'issued_date': 'Issued Jan 2017'}],\n",
       " 'projects': [{'project_name': 'Smart Sleep Sense: An Unobtrusive and IoT Enabled Wireless Sleep Monitoring System',\n",
       "   'duration': 'May 2015 - Present',\n",
       "   'description': 'The project aims at developing a portable Human monitoring device that detects the health parameters (heart pulse, breathing rate, thermal emotion recognition, awakening at REM stage of the sleep) and communicates the signals obtained from sensors to the Android app via Wireless network. The information is used to convey Smart Assistance to the user for their daily improved health care conditions thus improving their efficiency and longevity. We are offering sleep disorder monitoring devices. In addition to this, our business model is B2C and device will be directly available to the customer for purchase via online e-commerce stores as well as company e-store itself and the expected cost will be in between 110 to 149 dollar.'},\n",
       "  {'project_name': 'Optimized Fixed Point Kalman Filter Implementation on Embedded Hardware',\n",
       "   'duration': 'Aug 2013 - Apr 2014',\n",
       "   'description': 'Real time adaptive signal processing is an area of considerable interest for the researchers in the field of embedded signal processing. Owing to the large dynamic range and constant relative accuracy of floating point arithmetic, filters are traditionally realized in floating point data type. However, using floating point arithmetic in embedded systems lead to larger cost in terms of latency, power and memory. Therefore these algorithms need to be implemented in fixed point for optimizing these constraints. However with the reduction in the word length the precision get affected. Therefore these algorithms need to be coded properly with adequate fixed point format such that precision is not affected while the resources are optimized. In this thesis an attempt has been made to implement Least Mean Square (LMS), Recursive Least Square (RLS) and Kalman Filtering algorithms on ARM Cortex M0+ with Thumb instruction set. The word length has been optimized for accurate performance, reduced power, improved speed and reduced memory usage. The validation of the algorithm has been carried out with respect to MATLAB.'},\n",
       "  {'project_name': 'MIPS64 5-stages Processor Designing',\n",
       "   'duration': 'Jan 2013 - May 2013',\n",
       "   'description': 'The machine was designed with the MIPS instruction set in mind. Its purpose was to be able to take  a  binary  representation  of  a  MIPS  instruction  and  convert  it  into  microcode.    The microcode controls the actions of the processor and pipeline. Pipelining is a process of multitasking instructions in the same data path.  An instruction can be loaded into the data path at every clock cycle, even though each instruction takes up to 5 cycles to complete.  At every clock cycle, each instruction is stored into a different stage in the pipeline. Doing this does not affect any of the other instructions in the processor because they are in different stages. Each stage  contains  a  different  part  of  the  data  path  and  has  a specific function. This allows the user of the processor to fully utilize all components of the data path simultaneously, causing an increase in the throughput of their program.'},\n",
       "  {'project_name': 'Transistor Characteristics Curve Tracer',\n",
       "   'duration': 'Feb 2012 - May 2013',\n",
       "   'description': 'A cost effective and reliable curve tracer has been designed and developed for NPN- PNP transistors using AT89C51 microcontroller. The system plots desired number of transfer and static characteristics curves on a given range of constant biasing parameter viz; VCE and IB resp. The system plots these characteristics curves of the given transistor on the available computer system through the developed software on Visual BASIC 6.0.  h-parameters of the transistor have also been obtained for different design requirement. All the data has also been stored in a user defined output file for further analysis.'},\n",
       "  {'project_name': 'Automated BJT Curve Tracer using Visual Basic',\n",
       "   'duration': 'Jan 2012 - May 2012',\n",
       "   'description': 'The characteristic of any discrete component plays an important role for determining its behavior for different applications. In this paper we have developed a cost effective BJT curve tracer and binning system which consist of hardware part that is implemented using 89c51 microcontroller and software part which is developed in Microsoft Visual Basic 6.0. This complete system plots the input characteristics curves, h-parameters of the given transistor along with other parameters i.e. C-E resistance and early voltage. Binning machine is used to find the working mode of BJT and to find whether the BJT is working or not by using the port given on the hardware. All the data can also be stored in a user defined output file in the computer for further analysis. This tracer and binning machine can be used for educational and engineering purposes.'}],\n",
       " 'courses': [{'course_name': 'Advanced Computer Architecture',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Algorithm Design and Analysis',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Android Sensors Programming', 'associated_with': ''},\n",
       "  {'course_name': 'Computational Methods and Algorithm for Signal Processing',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Data Structure',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Deep Learning with TensorFlow 2.0 and Keras',\n",
       "   'associated_with': ''},\n",
       "  {'course_name': 'Digital Signal Processing',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Embedded System',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Embedded System Design and Validation',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Feature Selection in Machine Learning',\n",
       "   'associated_with': ''},\n",
       "  {'course_name': 'Internet of Things (IoT)',\n",
       "   'associated_with': 'Associated with IIT Kharagpur'},\n",
       "  {'course_name': 'Low Power System Design',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Machine Learning for Healthcare', 'associated_with': ''},\n",
       "  {'course_name': 'Natural Language Processing with SpaCy',\n",
       "   'associated_with': ''},\n",
       "  {'course_name': 'Programmable Embedded System',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'},\n",
       "  {'course_name': 'Python for Beginners', 'associated_with': ''},\n",
       "  {'course_name': 'Signal Processing with MATLAB', 'associated_with': ''},\n",
       "  {'course_name': 'Smart Phone and Cloud Computation',\n",
       "   'associated_with': 'Associated with Indian Institute of Technology, Kharagpur'}],\n",
       " 'honors_and_awards': ['Best Startup at IIT Kharagpur 2016-17',\n",
       "  'Runner-Up 24H Entrepreneurship Challenge, 2017, IISER, Pune',\n",
       "  'Runner-Up Texas Instrument Innovation Challenge 2016-17',\n",
       "  'Top 50 Innovators Award at India IIGP 2.0 Boot Camp, IIM Ahmedabad',\n",
       "  'Winner Accenture Innovation Challenge 2017',\n",
       "  'Winner Of International Business Model Competition, Eureka 2016']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.back()\n",
    "\n",
    "driver.find_element(By.ID, \"navigation-index-see-all-honorsandawards\").click()\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "soup = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "\n",
    "items = soup.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "item_list = []\n",
    "for item in items:\n",
    "    item_list.append(item.get_text().strip())\n",
    "\n",
    "profile_data['honors_and_awards'] = item_list\n",
    "\n",
    "profile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4146de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa472658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
